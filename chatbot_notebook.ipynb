{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with LangGraph and Groq\n",
    "\n",
    "This notebook demonstrates how to build a simple chatbot using LangGraph for workflow management and Groq for LLM integration.\n",
    "\n",
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell if packages are not installed)\n",
    "# !pip install langgraph langchain-groq langchain-core python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph import graph\n",
    "from langgraph.graph import add_messages, StateGraph, END\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM\n",
    "\n",
    "We'll use Groq's Llama3-8b-8192 model for fast inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "    temperature=0,  # Low temperature for more consistent responses\n",
    "    model=\"llama3-8b-8192\"  # Fast and efficient model\n",
    ")\n",
    "\n",
    "print(f\"\u2705 LLM initialized with model: {llm.model}\")\n",
    "print(f\"Temperature: {llm.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Chat State\n",
    "\n",
    "We use a TypedDict to define the structure of our chat state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicChatState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "print(\"\u2705 Chat state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Chatbot Node\n",
    "\n",
    "This function processes the chat state and generates a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: BasicChatState):\n",
    "    \"\"\"\n",
    "    Process the chat state and generate a response using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        state: The current chat state containing messages\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated state with the LLM response\n",
    "    \"\"\"\n",
    "    # Generate response using the LLM\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Chatbot function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LangGraph\n",
    "\n",
    "Now we'll create the workflow graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the state graph\n",
    "graph = StateGraph(BasicChatState)\n",
    "\n",
    "# Add the chatbot node\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Set the entry point\n",
    "graph.set_entry_point(\"chatbot\")\n",
    "\n",
    "# Add edge to end\n",
    "graph.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"\u2705 LangGraph workflow created and compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Chatbot\n",
    "\n",
    "Let's test our chatbot with a simple message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test message\n",
    "test_message = \"Hello! How are you today?\"\n",
    "\n",
    "print(f\"User: {test_message}\")\n",
    "\n",
    "# Invoke the chatbot\n",
    "results = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=test_message)]\n",
    "})\n",
    "\n",
    "print(f\"AI: {results['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Function\n",
    "\n",
    "Let's create a function for interactive chatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot():\n",
    "    \"\"\"\n",
    "    Interactive chat function that allows you to have a conversation with the bot.\n",
    "    Type 'exit' to quit.\n",
    "    \"\"\"\n",
    "    print(\"\ud83e\udd16 Chatbot is ready! Type 'exit' to quit.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\ud83d\udc4b Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            results = app.invoke({\n",
    "                \"messages\": [HumanMessage(content=user_input)]\n",
    "            })\n",
    "            \n",
    "            print(f\"\ud83e\udd16 Bot: {results['messages'][-1].content}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error: {e}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"\u2705 Interactive chat function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Interactive Chat\n",
    "\n",
    "Run the cell below to start chatting with your bot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to start interactive chat\n",
    "# chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. \u2705 **LangGraph Setup**: Creating a workflow graph for chat processing\n",
    "2. \u2705 **Groq Integration**: Using Groq's fast LLM models\n",
    "3. \u2705 **State Management**: Managing conversation state with TypedDict\n",
    "4. \u2705 **Interactive Chat**: Building a conversational interface\n",
    "5. \u2705 **Error Handling**: Robust error handling for production use\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Set up your `GROQ_API_KEY` in a `.env` file\n",
    "- Experiment with different models and temperatures\n",
    "- Add more complex conversation flows\n",
    "- Integrate with web interfaces or APIs\n",
    "\n",
    "Happy coding! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}